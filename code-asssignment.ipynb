{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10143318,"sourceType":"datasetVersion","datasetId":6260820}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:05:51.931849Z","iopub.execute_input":"2024-12-09T02:05:51.932693Z","iopub.status.idle":"2024-12-09T02:05:51.938078Z","shell.execute_reply.started":"2024-12-09T02:05:51.932645Z","shell.execute_reply":"2024-12-09T02:05:51.937389Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:41:04.686285Z","iopub.execute_input":"2024-12-09T01:41:04.686642Z","iopub.status.idle":"2024-12-09T01:41:05.592021Z","shell.execute_reply.started":"2024-12-09T01:41:04.686611Z","shell.execute_reply":"2024-12-09T01:41:05.591265Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T01:48:30.039769Z","iopub.execute_input":"2024-12-09T01:48:30.040101Z","iopub.status.idle":"2024-12-09T01:48:30.044141Z","shell.execute_reply.started":"2024-12-09T01:48:30.040072Z","shell.execute_reply":"2024-12-09T01:48:30.043288Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\n\ntrain_path = '/kaggle/input/codebert-finetuning/train.jsonl'\nval_path = '/kaggle/input/codebert-finetuning/valid.jsonl'\ntest_path = '/kaggle/input/codebert-finetuning/test.jsonl'\n\nos.system(\"cp /kaggle/input/codebert-finetuning/run.py .\")\nos.system(\"cp /kaggle/input/codebert-finetuning/model.py .\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:13:43.091698Z","iopub.execute_input":"2024-12-09T02:13:43.092252Z","iopub.status.idle":"2024-12-09T02:13:43.110742Z","shell.execute_reply.started":"2024-12-09T02:13:43.092220Z","shell.execute_reply":"2024-12-09T02:13:43.110013Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\nimport json\n\n\nfrom tqdm import tqdm, trange\nimport multiprocessing\nfrom model import Model\nfrom transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n                          RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n\nlogger = logging.getLogger(__name__)\n\nclass InputFeatures(object):\n    \"\"\"A single training/test features for a example.\"\"\"\n    def __init__(self,\n                 input_tokens,\n                 input_ids,\n                 label,\n\n    ):\n        self.input_tokens = input_tokens\n        self.input_ids = input_ids\n        self.label=label\n\n        \ndef convert_examples_to_features(js,tokenizer,args):\n    #source\n    code=' '.join(js['code'].split())\n    code_tokens=tokenizer.tokenize(code)[:args.block_size-2]\n    source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n    source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n    padding_length = args.block_size - len(source_ids)\n    source_ids+=[tokenizer.pad_token_id]*padding_length\n    return InputFeatures(source_tokens,source_ids,js['label'])\n\nclass TextDataset(Dataset):\n    def __init__(self, tokenizer, args, file_path=None):\n        self.examples = []\n        with open(file_path) as f:\n            for line in f:\n                js=json.loads(line.strip())\n                self.examples.append(convert_examples_to_features(js,tokenizer,args))\n        if 'train' in file_path:\n            for idx, example in enumerate(self.examples[:3]):\n                    logger.info(\"*** Example ***\")\n                    logger.info(\"label: {}\".format(example.label))\n                    logger.info(\"input_tokens: {}\".format([x.replace('\\u0120','_') for x in example.input_tokens]))\n                    logger.info(\"input_ids: {}\".format(' '.join(map(str, example.input_ids))))\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i):       \n        return torch.tensor(self.examples[i].input_ids),torch.tensor(self.examples[i].label)\n            \n\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYHTONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef train(args, train_dataset, model, tokenizer):\n    \"\"\" Train the model \"\"\" \n    train_sampler = RandomSampler(train_dataset)\n    \n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, \n                                  batch_size=args.train_batch_size,num_workers=4,pin_memory=True)\n    \n\n    \n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n         'weight_decay': args.weight_decay},\n        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    max_steps = len(train_dataloader) * args.num_train_epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=max_steps*0.1,\n                                                num_training_steps=max_steps)\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  batch size = %d\", args.train_batch_size)\n    logger.info(\"  Total optimization steps = %d\", max_steps)\n    best_acc=0.0\n    model.zero_grad()\n \n    for idx in range(args.num_train_epochs): \n        bar = tqdm(train_dataloader,total=len(train_dataloader))\n        losses=[]\n        for step, batch in enumerate(bar):\n            inputs = batch[0].to(args.device)        \n            labels=batch[1].to(args.device) \n            model.train()\n            loss,logits = model(inputs,labels)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            losses.append(loss.item())\n            bar.set_description(\"epoch {} loss {}\".format(idx,round(np.mean(losses),3)))\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()  \n                \n        results = evaluate(args, model, tokenizer)\n        for key, value in results.items():\n            logger.info(\"  %s = %s\", key, round(value,4))    \n            \n        # Save model checkpoint\n        if results['eval_acc']>best_acc:\n            best_acc=results['eval_acc']\n            logger.info(\"  \"+\"*\"*20)  \n            logger.info(\"  Best acc:%s\",round(best_acc,4))\n            logger.info(\"  \"+\"*\"*20)                          \n\n            checkpoint_prefix = 'checkpoint-best-acc'\n            output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))                        \n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)                        \n            model_to_save = model.module if hasattr(model,'module') else model\n            output_dir = os.path.join(output_dir, '{}'.format('model.bin')) \n            torch.save(model_to_save.state_dict(), output_dir)\n            logger.info(\"Saving model checkpoint to %s\", output_dir)\n                        \n\n\n\ndef evaluate(args, model, tokenizer):\n    eval_output_dir = args.output_dir\n\n    eval_dataset = TextDataset(tokenizer, args,args.eval_data_file)\n\n    if not os.path.exists(eval_output_dir):\n        os.makedirs(eval_output_dir)\n\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,num_workers=4,pin_memory=True)\n\n    # Eval!\n    logger.info(\"***** Running evaluation *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n    logits=[] \n    labels=[]\n    for batch in eval_dataloader:\n        inputs = batch[0].to(args.device)        \n        label=batch[1].to(args.device) \n        with torch.no_grad():\n            lm_loss,logit = model(inputs,label)\n            eval_loss += lm_loss.mean().item()\n            logits.append(logit.cpu().numpy())\n            labels.append(label.cpu().numpy())\n        nb_eval_steps += 1\n    logits=np.concatenate(logits,0)\n    labels=np.concatenate(labels,0)\n    preds=logits.argmax(-1)\n    eval_acc=np.mean(labels==preds)\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.tensor(eval_loss)\n            \n    result = {\n        \"eval_loss\": float(perplexity),\n        \"eval_acc\":round(eval_acc,4),\n    }\n    return result\n\ndef test(args, model, tokenizer):\n    # Note that DistributedSampler samples randomly\n    eval_dataset = TextDataset(tokenizer, args,args.test_data_file)\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n    # Eval!\n    logger.info(\"***** Running Test *****\")\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n    logits=[]   \n    labels=[]\n    for batch in tqdm(eval_dataloader,total=len(eval_dataloader)):\n        inputs = batch[0].to(args.device)        \n        label=batch[1].to(args.device) \n        with torch.no_grad():\n            logit = model(inputs)\n            logits.append(logit.cpu().numpy())\n            labels.append(label.cpu().numpy())\n\n    logits=np.concatenate(logits,0)\n    labels=np.concatenate(labels,0)\n    preds=logits.argmax(-1)\n    with open(os.path.join(args.output_dir,\"predictions.txt\"),'w') as f:\n        for example,pred in zip(eval_dataset.examples,preds):\n            if pred:\n                f.write('1\\n')\n            else:\n                f.write('0\\n')    \n    \n                        \n                        \ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(\"--train_data_file\", default=None, type=str, required=True,\n                        help=\"The input training data file (a text file).\")\n    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n\n    ## Other parameters\n    parser.add_argument(\"--eval_data_file\", default=None, type=str,\n                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n    parser.add_argument(\"--test_data_file\", default=None, type=str,\n                        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n    parser.add_argument(\"--model_name_or_path\", default=None, type=str,\n                        help=\"The model checkpoint for weights initialization.\")\n    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n                        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n    parser.add_argument(\"--block_size\", default=-1, type=int,\n                        help=\"Optional input sequence length after tokenization.\")\n    parser.add_argument(\"--do_train\", action='store_true',\n                        help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action='store_true',\n                        help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\"--do_test\", action='store_true',\n                        help=\"Whether to run eval on the dev set.\")    \n    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n                        help=\"Batch size per GPU/CPU for training.\")\n    parser.add_argument(\"--eval_batch_size\", default=4, type=int,\n                        help=\"Batch size per GPU/CPU for evaluation.\")\n    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n                        help=\"The initial learning rate for Adam.\")\n    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n                        help=\"Weight deay if we apply some.\")\n    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n                        help=\"Epsilon for Adam optimizer.\")\n    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n                        help=\"Max gradient norm.\")\n    parser.add_argument(\"--warmup_steps\", default=0, type=int,\n                        help=\"Linear warmup over warmup_steps.\")\n    parser.add_argument('--seed', type=int, default=42,\n                        help=\"random seed for initialization\")\n    parser.add_argument('--num_train_epochs', type=int, default=42,\n                        help=\"num_train_epochs\")\n\n    args = parser.parse_args()\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    args.n_gpu = torch.cuda.device_count()\n    \n    args.device = device\n    # Setup logging\n    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                        datefmt='%m/%d/%Y %H:%M:%S',\n                        level=logging.INFO)\n    logger.warning(\"device: %s, n_gpu: %s\", device, args.n_gpu)\n\n    # Set seed\n    set_seed(args.seed)\n\n    config = RobertaConfig.from_pretrained(args.model_name_or_path)\n    config.num_labels=100\n    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n    model = RobertaForSequenceClassification.from_pretrained(args.model_name_or_path,config=config)    \n\n    model=Model(model,config,tokenizer,args)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    model.to(args.device)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n        \n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = TextDataset(tokenizer, args,args.train_data_file)\n        train(args, train_dataset, model, tokenizer)\n\n    # Evaluation\n    results = {}\n    if args.do_eval:\n        checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n        model.load_state_dict(torch.load(output_dir))      \n        model.to(args.device)\n        result=evaluate(args, model, tokenizer)\n        logger.info(\"***** Eval results *****\")\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(round(result[key],4)))\n            \n    if args.do_test:\n        checkpoint_prefix = 'checkpoint-best-acc/model.bin'\n        output_dir = os.path.join(args.output_dir, '{}'.format(checkpoint_prefix))  \n        model.load_state_dict(torch.load(output_dir))                  \n        model.to(args.device)\n        test(args, model, tokenizer)\n\n    return results\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:08:58.323718Z","iopub.execute_input":"2024-12-09T02:08:58.324035Z","iopub.status.idle":"2024-12-09T02:09:10.144288Z","shell.execute_reply.started":"2024-12-09T02:08:58.324008Z","shell.execute_reply":"2024-12-09T02:09:10.143195Z"}},"outputs":[{"name":"stderr","text":"usage: ipykernel_launcher.py [-h] --train_data_file TRAIN_DATA_FILE\n                             --output_dir OUTPUT_DIR\n                             [--eval_data_file EVAL_DATA_FILE]\n                             [--test_data_file TEST_DATA_FILE]\n                             [--model_name_or_path MODEL_NAME_OR_PATH]\n                             [--tokenizer_name TOKENIZER_NAME]\n                             [--block_size BLOCK_SIZE] [--do_train]\n                             [--do_eval] [--do_test]\n                             [--train_batch_size TRAIN_BATCH_SIZE]\n                             [--eval_batch_size EVAL_BATCH_SIZE]\n                             [--learning_rate LEARNING_RATE]\n                             [--weight_decay WEIGHT_DECAY]\n                             [--adam_epsilon ADAM_EPSILON]\n                             [--max_grad_norm MAX_GRAD_NORM]\n                             [--warmup_steps WARMUP_STEPS] [--seed SEED]\n                             [--num_train_epochs NUM_TRAIN_EPOCHS]\nipykernel_launcher.py: error: the following arguments are required: --train_data_file, --output_dir\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"],"ename":"SystemExit","evalue":"2","output_type":"error"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\n\ntrain_path = '/kaggle/input/codebert-finetuning/train.jsonl'\nval_path = '/kaggle/input/codebert-finetuning/valid.jsonl'\ntest_path = '/kaggle/input/codebert-finetuning/test.jsonl'\n\nfor path in [train_path, val_path, test_path]:\n    print(f\"Exists: {os.path.exists(path)} - {path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:13:50.945435Z","iopub.execute_input":"2024-12-09T02:13:50.945769Z","iopub.status.idle":"2024-12-09T02:13:50.953459Z","shell.execute_reply.started":"2024-12-09T02:13:50.945742Z","shell.execute_reply":"2024-12-09T02:13:50.952634Z"}},"outputs":[{"name":"stdout","text":"Exists: True - /kaggle/input/codebert-finetuning/train.jsonl\nExists: True - /kaggle/input/codebert-finetuning/valid.jsonl\nExists: True - /kaggle/input/codebert-finetuning/test.jsonl\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!python run.py \\\n  --train_data_file /kaggle/input/codebert-finetuning/train.jsonl \\\n  --eval_data_file /kaggle/input/codebert-finetuning/valid.jsonl \\\n  --test_data_file /kaggle/input/codebert-finetuning/test.jsonl \\\n  --output_dir /kaggle/working/output_dir \\\n  --model_name_or_path microsoft/codebert-base \\\n  --tokenizer_name microsoft/codebert-base \\\n  --block_size 512 \\\n  --do_train \\\n  --do_eval \\\n  --do_test \\\n  --train_batch_size 8 \\\n  --eval_batch_size 8 \\\n  --learning_rate 5e-5 \\\n  --num_train_epochs 3 \\\n  --seed 42\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:14:49.935965Z","iopub.execute_input":"2024-12-09T02:14:49.936727Z","iopub.status.idle":"2024-12-09T02:39:21.642910Z","shell.execute_reply.started":"2024-12-09T02:14:49.936691Z","shell.execute_reply":"2024-12-09T02:39:21.641835Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n  0%|                                                   | 0/750 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nepoch 0 loss 0.471: 100%|█████████████████████| 750/750 [05:32<00:00,  2.25it/s]\nepoch 1 loss 0.001: 100%|█████████████████████| 750/750 [05:32<00:00,  2.25it/s]\nepoch 2 loss 0.0: 100%|███████████████████████| 750/750 [05:32<00:00,  2.25it/s]\n/kaggle/working/run.py:336: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(output_dir))\n/kaggle/working/run.py:346: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(output_dir))\n100%|█████████████████████████████████████████| 750/750 [01:27<00:00,  8.54it/s]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}